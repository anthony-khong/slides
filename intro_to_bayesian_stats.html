<!doctype html>
<html>
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

        <title>reveal.js</title>

        <link rel="stylesheet" href="css/reveal.css">
        <link rel="stylesheet" href="css/theme/simple.css">

        <!-- Theme used for syntax highlighting of code -->
        <link rel="stylesheet" href="lib/css/solarized.css">

        <!-- Printing and PDF exports -->
        <script>
            var link = document.createElement( 'link' );
            link.rel = 'stylesheet';
            link.type = 'text/css';
            link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
            document.getElementsByTagName( 'head' )[0].appendChild( link );
        </script>
    </head>
    <body>
        <div class="reveal"> <div class="slides">
            <section id="title_page">
                <h2><span style="color:slateblue">Introduction to Bayesian Statistics and Computations</span></h2>
                <h4>Anthony Khong</h4>
                <h4>23 May 2018</h4>
            </section>

            <section id="contents_page">
                <ol>
                    <li>Bayesian Statistics</li>
                    <ul>
                        <li>Definition of Probability</li>
                        <li>Bayesian Inference</li>
                        <li>Statistical Decision Theory</li>
                    </ul>
                    <li>Bayesian Computations</li>
                    <ul>
                        <li>Analytical Solutions</li>
                        <li>Simulation Methods</li>
                        <li>Variational Methods</li>
                        <li>Approximate Bayesian Computations</li>
                    </ul>
                </ol>
            </section>

            <section id="bayesian_statistics">
                <section>
                    <h2><span style="color:slateblue">1. Bayesian Statitics</span></h2>
                </section>
                <section>
                    <h3><span style="color:slateblue">1.1 Definition of Probability</span></h3>
                </section>
                <section>
                    <p>Suppose I'm going to flip a coin.</p>
                    <p>What is $\mathbb{P}(Head)$?</p>
                </section>
                <section>
                    <p>Suppose I've flipped a coin.</p>
                    <p>What is $\mathbb{P}(Head)$?</p>
                </section>
                <section>
                    <p>What is probability?</p>
                    <p class="fragment fade-up">Frequentist: limiting relative frequency of an event.</p>
                    <p class="fragment fade-up">Bayesian: degree of belief of an event.</p>
                </section>
                <section>
                    <h3><span style="color:slateblue">1.2 Bayesian Inference</span></h3>
                </section>
                <section>
                    <p></p>
                    $$Y_i \sim \mathcal{N}(\mu, \sigma^2)$$
                    <p></p>
                    <p>How do we <b>infer</b> $(\mu, \sigma^2)$ from $\{y_i\}_{i=1}^N$?</p>
                    <aside class="notes">
                        Notation: capital letters for the random variables and small letters for their realisations.
                    </aside>
                </section>
                <section>
                    <h3><span style="color:slateblue">Frequentist Solution</span></h3>
                    <p></p>
                    $$
                    \hat{\mu} = \dfrac{1}{N}\sum_i Y_i \quad
                    \hat{\sigma}^2 = \dfrac{1}{N}\sum_i (Y_i - \hat{\mu})^2
                    $$
                    <p class="fragment fade-up">
                        Find <b>estimators</b> whose sampling distributions are close to the unknown true parameters.
                    </p>
                    <aside class="notes">
                        Under repeated sampling, the true value lies somewhere in a region of high density.

                        The output is a random variable for each parameter, for which we have only one realisation.
                    </aside>
                </section>
                <section>
                    <h3><span style="color:slateblue">Bayesian Solution</span></h3>
                    <p></p>
                    $$
                    \pi(\mu, \sigma^2 | \boldsymbol y)
                        = \dfrac{p(\boldsymbol y | \mu, \sigma^2) \pi(\mu, \sigma^2)}{p(\boldsymbol y)}
                    $$
                    <p class="fragment fade-up">
                        Formulate a <b>posterior belief</b> of the unknown parameters by refining prior beliefs with data.
                    </p>
                    <aside class="notes">
                        We refine beliefs when we see more data. In some sense, this statement cannot be false.

                        The output is a distribution over each parameter.
                    </aside>
                </section>
                <section>
                    <h3><span style="color:slateblue">The Bayesian Recipe</span></h3>
                    <p class="fragment fade-up">
                        Parameters of Interest with Prior:
                        $$\boldsymbol\theta \sim \pi(\boldsymbol \theta)$$
                    </p>
                    <p class="fragment fade-up">
                        Observed Data with Likelihood:
                        $$ \mathcal{D} | \boldsymbol \theta \sim p(\mathcal{D} | \boldsymbol \theta)$$
                    </p>
                    <p class="fragment fade-up">
                        Posterior distribution by Bayes Theorem:
                        $$\pi(\boldsymbol \theta | \mathcal{D}) \propto p(\mathcal{D} | \boldsymbol \theta) \pi(\boldsymbol\theta)$$
                    </p>
                </section>
                <section>
                    <h3><span style="color:slateblue">1.3 Statistical Decision Theory</span></h3>
                </section>
                <section>
                    <p>How do you turn $\pi(\boldsymbol \theta | \mathcal{D})$ into $\hat{\theta}$ ?</p>
                    <p></p>
                    <p class="fragment fade-up">
                        $$
                        \mathbb{E}\Big(L(\boldsymbol\theta, \tilde{\boldsymbol \theta})\Big)
                            = \int_{\boldsymbol\theta} L(\boldsymbol\theta, \tilde{\boldsymbol \theta})
                              \pi(\boldsymbol \theta | \mathcal{D}) d\boldsymbol\theta
                        $$
                    </p>
                    <p></p>
                    <p class="fragment fade-up">
                        $$
                        \hat{\boldsymbol \theta}
                            = \arg \min_{\tilde{\boldsymbol \theta}}
                            \mathbb{E}\Big(L(\boldsymbol\theta, \tilde{\boldsymbol \theta})\Big)
                        $$
                    </p>
                    <aside class="notes">
                        This recipe is not dependent on sample size, because we never need to invoke asymptotics.
                    </aside>
                </section>
                <section>
                    <p class="fragment fade-up">
                        $$
                        L(\boldsymbol\theta, \tilde{\boldsymbol \theta})
                        = \sum_{j=1}^{J}(\theta_j - \tilde{\theta}_j)^2
                        \,\Rightarrow\, \hat{\boldsymbol \theta}
                        = mean_{\boldsymbol\theta | \mathcal{D}} (\boldsymbol\theta)
                        $$
                    </p>
                    <p class="fragment fade-up">
                        $$
                        L(\boldsymbol\theta, \tilde{\boldsymbol \theta})
                        = \sum_{j=1}^{J}|\boldsymbol\theta_j - \tilde{\boldsymbol \theta}_j|
                        \,\Rightarrow\, \hat{\boldsymbol \theta}
                        = median_{\boldsymbol\theta | \mathcal{D}} (\boldsymbol\theta)
                        $$
                    </p>
                    <p class="fragment fade-up">
                        $$
                        L(\boldsymbol\theta, \tilde{\boldsymbol \theta})
                        = \mathbb{1}(\boldsymbol\theta = \tilde{\boldsymbol\theta})
                        \,\Rightarrow\, \hat{\boldsymbol \theta}
                        = mode_{\boldsymbol\theta | \mathcal{D}} (\boldsymbol\theta)
                        $$
                    </p>
                </section>
                <section>
                    <h3><span style="color:slateblue">Optimal Action</span></h3>
                    <p> Suppose we have historical data and actions $(\mathcal{D}, \boldsymbol a)$:</p>
                    $$
                    \pi(\boldsymbol \theta | \mathcal{D}, \boldsymbol a)
                    \propto p(\mathcal{D} | \boldsymbol\theta, \boldsymbol a) \pi(\boldsymbol \theta)
                    $$
                    <p>How should we pick the next action $a'$?</p>
                    <p class="fragment fade-up">
                    <small>
                        $$
                        \mathbb{E}\Big(L(\mathcal{D}', a')\Big)
                        = \int_{\mathcal{D}'} \int_{\boldsymbol\theta}
                        L(\mathcal{D}', a') p(\mathcal{D}' | \boldsymbol \theta, a')
                        \pi(\boldsymbol \theta | \mathcal{D}, \boldsymbol a) d\boldsymbol\theta d\mathcal{D}'
                        $$
                    </small>
                    <p class="fragment fade-up">
                        $$
                        a'^* = \arg \min_{a'} \mathbb{E}\Big(L(\mathcal{D}', a')\Big)
                        $$
                    </p>
                    </p>
                    <aside class="notes">
                        Often we do not care about the parameter values or choosing its value. How do we use this to pick actions?

                        If you're wondering why TF Probability is a big deal...
                        It lets you think at this high level instead of the actual components of these functions.
                        Conceptually, this is very easy. Could you imagine what you need to do in the Frequentist framework?
                    </aside>
                </section>
            </section>

            <section>
                <section>
                    <h2><span style="color:slateblue">2. Bayesian Computations</span></h2>
                </section>
                <section>
                    $$
                    \mathbb{E}_{\boldsymbol\theta | \mathcal{D}}\Big(\phi(\boldsymbol\theta)\Big)
                    = \int_{\boldsymbol\theta} \phi(\boldsymbol \theta) \pi(\boldsymbol \theta | \mathcal{D}) d \boldsymbol\theta
                    $$
                </section>
                <section>
                    <h3><span style="color:slateblue">2.1 Analytical Solutions</span></h3>
                </section>
                <section>
                    <p>$$Y_i | \mu, \sigma^2 \sim \mathcal{N}(\mu, \sigma^2)$$</p>

                    <p class="fragment fade-up">
                        $$\sigma^{-2} \sim \mathcal{G}(\alpha_0, \beta_0) \quad \mu | \sigma^2 \sim \mathcal{N}(\mu_0, \lambda_0 \sigma^2)$$
                    </p>

                    <p>
                    $$
                    \pi(\mu, \sigma^2 | \boldsymbol y)
                        = \dfrac{p(\boldsymbol y | \mu, \sigma^2) \pi(\mu, \sigma^2)}{p(\boldsymbol y)}
                    $$
                    </p>

                    <p class="fragment fade-up">
                        TODO
                    </p>
                </section>
                <section>
                    <h3><span style="color:slateblue">Linear Regression with Conjugate Priors</span></h3>
                    <img data-src="figures/bayesian_linear_regression_conjugate_prior.png" width="85%">
                </section>
                <section>
                    <h3><span style="color:slateblue">2.2 Simulation Methods</span></h3>
                </section>
                <section>
                    <h3><span style="color:slateblue">Monte Carlo Integration</span></h3>
                    $$\mathbb{E}(\phi) = \int_{\boldsymbol\theta} f(\boldsymbol \theta) \pi(\boldsymbol \theta | \mathcal{D}) d \boldsymbol\theta$$
                    <p>Suppose we have a sample $\{\boldsymbol\theta_m\}_{m=1}^M$ from $\pi(\boldsymbol\theta | \mathcal{D})$:</p>

                    <p class="fragment fade-up">
                    $$
                    \hat{\phi}
                    = \int_{\boldsymbol\theta} \phi(\boldsymbol \theta)
                    \Big\{ \sum_{m=1}^M \delta_{\boldsymbol\theta_m}(\boldsymbol\theta) \Big\} d \boldsymbol\theta
                    = \sum_{m=1}^M \phi(\boldsymbol \theta_m)
                    $$
                    </p>

                    <p class="fragment fade-up">
                    <b>LLN:</b> if $\mathbb{E}(\phi)$ and $\mathbb{V}(\phi)$ are both finite, then
                    </p>
                    <p class="fragment fade-up">
                    $$
                    \sqrt{\dfrac{M}{\mathbb{V}(\phi)}}\Big(\hat{\phi} - \mathbb{E}(\phi)\Big)
                    \overset{D}{\rightarrow} \mathcal{N}(0, 1)
                    $$
                    </p>
                </section>
                <section>
                    <h3><span style="color:slateblue">Rejection Sampling</span></h3>

                    <p>Suppose we have $q(\boldsymbol\theta)$ and $C \in \mathbb{R}^+$ where</p>
                    $$\dfrac{\pi(\boldsymbol\theta | \mathcal{D})}{Cq(\boldsymbol\theta)} \leq 1 \quad \, \forall \boldsymbol\theta$$

                    <p></p>
                    <ol class="fragment fade-up">
                        <li>Draw $\boldsymbol\theta \sim q$</li>
                        <li>Accept $\boldsymbol\theta$ w.p. $\dfrac{\pi(\boldsymbol\theta | \mathcal{D})}{Cq(\boldsymbol\theta)}$</li>
                    </ol>
                </section>
            </section>

        </div> </div>
        <script src="lib/js/head.min.js"></script>
        <script src="js/reveal.js"></script>

        <script>
            // More info about config & dependencies:
            // - https://github.com/hakimel/reveal.js#configuration
            // - https://github.com/hakimel/reveal.js#dependencies
            Reveal.initialize({
                transition: "none",
                history: true,
                math: {
                    mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
                    config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
                },
                dependencies: [
                    { src: 'plugin/markdown/marked.js' },
                    { src: 'plugin/markdown/markdown.js' },
                    { src: 'plugin/notes/notes.js', async: true },
                    { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
                    { src: 'plugin/math/math.js', async: true }
                ]
            });
        </script>
    </body>
</html>
